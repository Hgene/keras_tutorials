{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32768)             1081344   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 256)       819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 32, 32, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 32, 32, 3)         37635     \n",
      "=================================================================\n",
      "Total params: 6,264,579\n",
      "Trainable params: 6,264,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "\n",
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "generator_input = keras.Input(shape = (latent_dim,))\n",
    "\n",
    "x = layers.Dense(128 * 16 * 16)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((16, 16, 128))(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(256, 4, strides = 2,padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Gene\\Anaconda3\\envs\\tensor\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 30, 30, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 790,913\n",
      "Trainable params: 790,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator_input= layers.Input(shape=(height, width, channels))\n",
    "\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "x = layers.Dropout(0.4)(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n",
    "\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(lr = 0.0008, clipvalue=1.0, decay=1e-8)\n",
    "discriminator.compile(optimizer=discriminator_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = keras.models.Model(gan_input, gan_output)\n",
    "\n",
    "gan_optimizer = keras.optimizers.RMSprop(lr = 0.0004, clipvalue=1.0, decay=1e-8)\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Loss :  5.837258\n",
      "Generator Loss :  15.838412\n",
      "Discriminator Loss :  0.68996155\n",
      "Generator Loss :  0.8404046\n",
      "Discriminator Loss :  0.6902399\n",
      "Generator Loss :  0.75569856\n",
      "Discriminator Loss :  0.69421023\n",
      "Generator Loss :  0.72581464\n",
      "Discriminator Loss :  0.7002041\n",
      "Generator Loss :  0.73735046\n",
      "Discriminator Loss :  0.70198095\n",
      "Generator Loss :  0.7600259\n",
      "Discriminator Loss :  0.69560164\n",
      "Generator Loss :  0.79375285\n",
      "Discriminator Loss :  0.7031665\n",
      "Generator Loss :  0.73499405\n",
      "Discriminator Loss :  0.7577814\n",
      "Generator Loss :  0.782547\n",
      "Discriminator Loss :  0.6863969\n",
      "Generator Loss :  0.761431\n",
      "Discriminator Loss :  0.6928275\n",
      "Generator Loss :  0.640272\n",
      "Discriminator Loss :  0.6993106\n",
      "Generator Loss :  0.76769495\n",
      "Discriminator Loss :  0.7003769\n",
      "Generator Loss :  0.7580543\n",
      "Discriminator Loss :  0.7051937\n",
      "Generator Loss :  0.73157936\n",
      "Discriminator Loss :  0.7021839\n",
      "Generator Loss :  0.73207724\n",
      "Discriminator Loss :  0.69423753\n",
      "Generator Loss :  0.7649166\n",
      "Discriminator Loss :  0.6878498\n",
      "Generator Loss :  0.73957145\n",
      "Discriminator Loss :  0.6934221\n",
      "Generator Loss :  0.6478334\n",
      "Discriminator Loss :  0.70474434\n",
      "Generator Loss :  0.7633031\n",
      "Discriminator Loss :  0.69884825\n",
      "Generator Loss :  0.8147384\n",
      "Discriminator Loss :  0.6947298\n",
      "Generator Loss :  0.7607986\n",
      "Discriminator Loss :  0.71236193\n",
      "Generator Loss :  0.79361236\n",
      "Discriminator Loss :  0.69202805\n",
      "Generator Loss :  0.76185894\n",
      "Discriminator Loss :  0.70188296\n",
      "Generator Loss :  0.7434383\n",
      "Discriminator Loss :  0.6945586\n",
      "Generator Loss :  0.74719363\n",
      "Discriminator Loss :  0.69652236\n",
      "Generator Loss :  0.7565107\n",
      "Discriminator Loss :  0.69114625\n",
      "Generator Loss :  0.7754455\n",
      "Discriminator Loss :  0.69068384\n",
      "Generator Loss :  0.72052634\n",
      "Discriminator Loss :  0.6831338\n",
      "Generator Loss :  0.7512268\n",
      "Discriminator Loss :  0.70181936\n",
      "Generator Loss :  0.76910585\n",
      "Discriminator Loss :  0.6937011\n",
      "Generator Loss :  0.7712786\n",
      "Discriminator Loss :  0.6989099\n",
      "Generator Loss :  0.75099164\n",
      "Discriminator Loss :  0.6913034\n",
      "Generator Loss :  0.7433431\n",
      "Discriminator Loss :  0.69654495\n",
      "Generator Loss :  0.7719854\n",
      "Discriminator Loss :  0.71240646\n",
      "Generator Loss :  0.7463836\n",
      "Discriminator Loss :  0.68670464\n",
      "Generator Loss :  0.73772764\n",
      "Discriminator Loss :  0.6920224\n",
      "Generator Loss :  0.756812\n",
      "Discriminator Loss :  0.7017251\n",
      "Generator Loss :  0.7453147\n",
      "Discriminator Loss :  0.6884227\n",
      "Generator Loss :  0.7327296\n",
      "Discriminator Loss :  0.68498266\n",
      "Generator Loss :  0.77251405\n",
      "Discriminator Loss :  0.6915644\n",
      "Generator Loss :  0.7725209\n",
      "Discriminator Loss :  0.6857011\n",
      "Generator Loss :  0.7551529\n",
      "Discriminator Loss :  0.68420285\n",
      "Generator Loss :  0.81450826\n",
      "Discriminator Loss :  0.6868765\n",
      "Generator Loss :  0.7442005\n",
      "Discriminator Loss :  0.6949169\n",
      "Generator Loss :  0.72141516\n",
      "Discriminator Loss :  0.6979877\n",
      "Generator Loss :  0.7479204\n",
      "Discriminator Loss :  0.7018409\n",
      "Generator Loss :  0.79082584\n",
      "Discriminator Loss :  0.6991269\n",
      "Generator Loss :  0.77918744\n",
      "Discriminator Loss :  0.6823734\n",
      "Generator Loss :  1.19758\n",
      "Discriminator Loss :  0.6806366\n",
      "Generator Loss :  0.6971925\n",
      "Discriminator Loss :  0.6862945\n",
      "Generator Loss :  0.7135763\n",
      "Discriminator Loss :  0.6989939\n",
      "Generator Loss :  0.74056953\n",
      "Discriminator Loss :  0.69282544\n",
      "Generator Loss :  0.76811874\n",
      "Discriminator Loss :  0.6929317\n",
      "Generator Loss :  0.7722454\n",
      "Discriminator Loss :  0.6978887\n",
      "Generator Loss :  0.80932903\n",
      "Discriminator Loss :  0.69437134\n",
      "Generator Loss :  0.7241663\n",
      "Discriminator Loss :  0.68433046\n",
      "Generator Loss :  0.73777044\n",
      "Discriminator Loss :  0.6909416\n",
      "Generator Loss :  0.73160344\n",
      "Discriminator Loss :  0.718227\n",
      "Generator Loss :  0.80169505\n",
      "Discriminator Loss :  0.6939188\n",
      "Generator Loss :  0.74927866\n",
      "Discriminator Loss :  0.6907196\n",
      "Generator Loss :  0.72395766\n",
      "Discriminator Loss :  0.6976517\n",
      "Generator Loss :  0.75468177\n",
      "Discriminator Loss :  0.68613446\n",
      "Generator Loss :  0.7739587\n",
      "Discriminator Loss :  0.6935941\n",
      "Generator Loss :  0.7440999\n",
      "Discriminator Loss :  0.70121956\n",
      "Generator Loss :  0.7424494\n",
      "Discriminator Loss :  0.7230603\n",
      "Generator Loss :  0.7601838\n",
      "Discriminator Loss :  0.68890154\n",
      "Generator Loss :  0.7367902\n",
      "Discriminator Loss :  0.72863424\n",
      "Generator Loss :  0.71834517\n",
      "Discriminator Loss :  0.6848251\n",
      "Generator Loss :  0.73018664\n",
      "Discriminator Loss :  0.69607985\n",
      "Generator Loss :  0.79051864\n",
      "Discriminator Loss :  0.6928295\n",
      "Generator Loss :  0.7900909\n",
      "Discriminator Loss :  0.70467776\n",
      "Generator Loss :  0.726058\n",
      "Discriminator Loss :  0.6911706\n",
      "Generator Loss :  0.75584406\n",
      "Discriminator Loss :  0.6836381\n",
      "Generator Loss :  0.81405556\n",
      "Discriminator Loss :  0.7038893\n",
      "Generator Loss :  0.74603164\n",
      "Discriminator Loss :  0.6989857\n",
      "Generator Loss :  0.764655\n",
      "Discriminator Loss :  0.69519174\n",
      "Generator Loss :  0.7127396\n",
      "Discriminator Loss :  0.70301723\n",
      "Generator Loss :  0.78200334\n",
      "Discriminator Loss :  0.67828953\n",
      "Generator Loss :  0.75149655\n",
      "Discriminator Loss :  0.6909728\n",
      "Generator Loss :  0.7089268\n",
      "Discriminator Loss :  0.7020062\n",
      "Generator Loss :  0.7372045\n",
      "Discriminator Loss :  0.70571935\n",
      "Generator Loss :  0.76082325\n",
      "Discriminator Loss :  0.71014196\n",
      "Generator Loss :  0.7198071\n",
      "Discriminator Loss :  0.7586058\n",
      "Generator Loss :  0.8168856\n",
      "Discriminator Loss :  0.69689214\n",
      "Generator Loss :  0.7514713\n",
      "Discriminator Loss :  0.7073915\n",
      "Generator Loss :  0.772155\n",
      "Discriminator Loss :  0.707026\n",
      "Generator Loss :  0.8064082\n",
      "Discriminator Loss :  0.69782674\n",
      "Generator Loss :  0.78384113\n",
      "Discriminator Loss :  0.68811035\n",
      "Generator Loss :  0.7333244\n",
      "Discriminator Loss :  0.696987\n",
      "Generator Loss :  0.7784525\n",
      "Discriminator Loss :  0.6998044\n",
      "Generator Loss :  0.77648413\n",
      "Discriminator Loss :  0.7012335\n",
      "Generator Loss :  0.7821727\n",
      "Discriminator Loss :  0.7070431\n",
      "Generator Loss :  0.77739894\n",
      "Discriminator Loss :  0.69797766\n",
      "Generator Loss :  0.8424002\n",
      "Discriminator Loss :  0.700045\n",
      "Generator Loss :  0.6499497\n",
      "Discriminator Loss :  0.69246805\n",
      "Generator Loss :  0.7580103\n",
      "Discriminator Loss :  0.70538175\n",
      "Generator Loss :  0.79622406\n",
      "Discriminator Loss :  0.7314078\n",
      "Generator Loss :  0.8190843\n",
      "Discriminator Loss :  0.6607466\n",
      "Generator Loss :  0.7394716\n",
      "Discriminator Loss :  0.6873151\n",
      "Generator Loss :  0.74654317\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "\n",
    "(x_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train[y_train.flatten() == 6] #Choose images that class is 6 which is frog\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0],) + (height, width, channels)).astype('float32') / 255.\n",
    "\n",
    "iterations = 10000\n",
    "batch_size = 20\n",
    "save_dir = 'C:/Users/Gene/Desktop/projects/keras_tutorial/datasets/CIFAR10'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n",
    "start = 0\n",
    "\n",
    "for step in range(iterations):\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    \n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    \n",
    "    stop = start + batch_size\n",
    "    real_images = x_train[start:stop]\n",
    "    combined_imgaes = np.concatenate([generated_images, real_images])\n",
    "    \n",
    "    labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
    "    labels += 0.05 * np.random.random(labels.shape)\n",
    "    \n",
    "    d_loss = discriminator.train_on_batch(combined_imgaes, labels)\n",
    "    \n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    misleading_targets = np.zeros((batch_size,1))\n",
    "    \n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    start += batch_size\n",
    "    \n",
    "    if start > len(x_train) - batch_size:\n",
    "        start = 0\n",
    "    if step % 100 == 0:\n",
    "        gan.save_weights('gan.h5')\n",
    "        \n",
    "        print('Discriminator Loss : ', d_loss)\n",
    "        print('Generator Loss : ', a_loss)\n",
    "        \n",
    "        img = image.array_to_img(generated_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'generated_frog' + str(step) + '.png'))\n",
    "        \n",
    "        img = image.array_to_img(real_images[0] * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'real_frog' + str(step) + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor-gpu",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
